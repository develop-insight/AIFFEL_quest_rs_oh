{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "!bash Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab190912.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from konlpy.tag import Mecab\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#1.baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "train_df1 = pd.read_csv(\"train.csv\")\n",
        "train_df2 = pd.read_csv(\"eda_train.csv\")\n",
        "valid_df = pd.read_csv(\"valid.csv\")\n",
        "\n",
        "# 병합\n",
        "train_df = pd.concat([train_df1, train_df2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mecab = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    sentence = re.sub(r\"[^a-z0-9가-힣\\.!\\?\\s]\", \"\", sentence)\n",
        "    return mecab.morphs(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df[\"Q_\"] = df[\"Q\"].apply(preprocess)\n",
        "# df[\"A_\"] = df[\"A\"].apply(preprocess)\n",
        "# token_leng1 = df[\"Q_\"].apply(lambda x: len(x))\n",
        "# token_leng2 = df[\"A_\"].apply(lambda x: len(x))\n",
        "\n",
        "# plt.figure(figsize=(14, 6))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.hist(token_leng1, bins=50, color=\"lightcoral\", edgecolor=\"black\")\n",
        "# plt.title(\"question distribution\")\n",
        "# plt.xlabel(\"number of tokens\")\n",
        "# plt.ylabel(\"count\")\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.hist(token_leng2, bins=50, color=\"mediumseagreen\", edgecolor=\"black\")\n",
        "# plt.title(\"answer distribution\")\n",
        "# plt.xlabel(\"number of tokens\")\n",
        "# plt.ylabel(\"count\")\n",
        "# plt.grid(True)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_corpus(df):\n",
        "    df.dropna(inplace=True)\n",
        "    df[\"Q_\"] = df[\"Q\"].apply(preprocess)\n",
        "    df[\"A_\"] = df[\"A\"].apply(preprocess)\n",
        "    df.drop_duplicates(subset=[\"Q_\"], inplace=True)\n",
        "    df.drop_duplicates(subset=[\"A_\"], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    que_corpus, ans_corpus = [], []\n",
        "    for i in range(len(df)):\n",
        "        if len(df[\"Q_\"][i]) < 28 and len(df[\"A_\"][i]) < 35:\n",
        "            que_corpus.append(df[\"Q_\"][i])\n",
        "            ans_corpus.append([\"<SOS>\"] + df[\"A_\"][i] + [\"<EOS>\"])\n",
        "    return que_corpus, ans_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "que_corpus_train, ans_corpus_train = build_corpus(train_df)\n",
        "que_corpus_valid, ans_corpus_valid = build_corpus(valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 분포 시각화\n",
        "token_leng1 = pd.Series(que_corpus_train).apply(lambda x: len(x))\n",
        "token_leng2 = pd.Series(ans_corpus_train).apply(lambda x: len(x))\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(token_leng1, bins=50, color=\"lightcoral\", edgecolor=\"black\")\n",
        "plt.title(\"question distribution\")\n",
        "plt.xlabel(\"number of tokens\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(token_leng2, bins=50, color=\"mediumseagreen\", edgecolor=\"black\")\n",
        "plt.title(\"answer distribution\")\n",
        "plt.xlabel(\"number of tokens\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vocab_coverage(corpus, topk_list=[1000, 2000, 5000, 8000, 10000, 20000]):\n",
        "    tokenizer = Tokenizer(filters=\"\", oov_token=None)\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    word_counts = tokenizer.word_counts  # collections.OrderedDict\n",
        "    sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    total_tokens = sum([count for _, count in sorted_counts])\n",
        "\n",
        "    print(f\"총 토큰 수: {total_tokens}\")\n",
        "    print(f\"전체 단어 수 (고유): {len(sorted_counts)}\")\n",
        "\n",
        "    cumulative = np.cumsum([count for _, count in sorted_counts])\n",
        "    coverage_list = [\n",
        "        cumulative[k - 1] / total_tokens * 100 if k <= len(cumulative) else 100.0\n",
        "        for k in topk_list\n",
        "    ]\n",
        "\n",
        "    for k, cov in zip(topk_list, coverage_list):\n",
        "        print(f\"Vocab Size = {k:5d} → Coverage: {cov:.2f}%\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(np.arange(1, len(cumulative) + 1), cumulative / total_tokens * 100)\n",
        "    plt.xlabel(\"Vocab Size (Top-N Words)\")\n",
        "    plt.ylabel(\"Coverage (%)\")\n",
        "    plt.title(\"Vocab Size vs. Token Coverage\")\n",
        "    plt.grid(True)\n",
        "    plt.axhline(95, color=\"r\", linestyle=\"--\", label=\"95% Cutoff\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 코퍼스 기준 커버리지 확인\n",
        "compute_vocab_coverage(que_corpus_train + ans_corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(corpus1, corpus2, vocab_size=None, oov_token=\"<OOV>\"):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, filters=\"\")\n",
        "    tokenizer.fit_on_texts(corpus1 + corpus2)\n",
        "\n",
        "    tensor1 = tokenizer.texts_to_sequences(corpus1)\n",
        "    tensor2 = tokenizer.texts_to_sequences(corpus2)\n",
        "\n",
        "    tensor1 = tf.keras.preprocessing.sequence.pad_sequences(tensor1, padding=\"post\")\n",
        "    tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor2, padding=\"post\")\n",
        "\n",
        "    return tensor1, tensor2, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_corpus, dec_corpus, tokenizer = tokenize(que_corpus_train, ans_corpus_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_with_existing_tokenizer(corpus1, corpus2, tokenizer):\n",
        "    tensor1 = tokenizer.texts_to_sequences(corpus1)\n",
        "    tensor2 = tokenizer.texts_to_sequences(corpus2)\n",
        "\n",
        "    tensor1 = tf.keras.preprocessing.sequence.pad_sequences(tensor1, padding=\"post\")\n",
        "    tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor2, padding=\"post\")\n",
        "\n",
        "    return tensor1, tensor2\n",
        "\n",
        "enc_valid, dec_valid = tokenize_with_existing_tokenizer(que_corpus_valid, ans_corpus_valid, tokenizer)\n",
        "\n",
        "# tf.data.Dataset 구성\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((enc_corpus, dec_corpus))\n",
        "train_dataset = train_dataset.shuffle(len(enc_corpus)).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((enc_valid, dec_valid))\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def generate_lookahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_enc_mask = generate_padding_mask(src)\n",
        "\n",
        "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
        "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
        "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask\n",
        "\n",
        "\n",
        "# Multi Head Attention 구현\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_qk += mask * -1e9\n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask\n",
        "        )\n",
        "\n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
        "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, enc_attn\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        # Q, K, V 순서에 주의하세요!\n",
        "        out, dec_enc_attn = self.enc_dec_attn(\n",
        "            Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask\n",
        "        )\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "\n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "\n",
        "        return out, enc_attns\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
        "        out = x\n",
        "\n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = self.dec_layers[i](\n",
        "                out, enc_out, dec_enc_mask, padding_mask\n",
        "            )\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers,\n",
        "        d_model,\n",
        "        n_heads,\n",
        "        d_ff,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        pos_len,\n",
        "        dropout=0.2,\n",
        "        shared_fc=True,\n",
        "        shared_emb=False,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = tf.keras.layers.Embedding(\n",
        "                src_vocab_size, d_model\n",
        "            )\n",
        "        else:\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared_fc = shared_fc\n",
        "\n",
        "        if shared_fc:\n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared_fc:\n",
        "            out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.do(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        dec_out, dec_attns, dec_enc_attns = self.decoder(\n",
        "            dec_in, enc_out, dec_enc_mask, dec_mask\n",
        "        )\n",
        "\n",
        "        logits = self.fc(dec_out)\n",
        "\n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = step**-0.5\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return (self.d_model**-0.5) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
        "    gold = tgt[\n",
        "        :, 1:\n",
        "    ]  # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "            src, tgt_in, enc_mask, dec_enc_mask, dec_mask\n",
        "        )\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def eval_step(src, tgt, model):\n",
        "    tgt_in = tgt[:, :-1]\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "        src, tgt_in, enc_mask, dec_enc_mask, dec_mask\n",
        "    )\n",
        "\n",
        "    loss = loss_function(gold, predictions)\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.stop_training = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, current):\n",
        "        if self.best is None or current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stop_training = True\n",
        "                self.stopped_epoch = epoch\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_graph(train_log, test_log):\n",
        "    epochs = range(1, len(train_log) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, train_log, label=\"Train Loss\")\n",
        "    plt.plot(epochs, test_log, label=\"Test Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Train/Test Loss per Epoch\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, tokenizer, input_text, max_length=35, sos_id=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    Greedy decoding을 사용하여 입력 텍스트에 대한 응답을 생성합니다.\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Transformer 모델\n",
        "        tokenizer: 텍스트 토크나이저\n",
        "        input_text: 입력 텍스트 (문자열)\n",
        "        max_length: 생성할 최대 시퀀스 길이\n",
        "        sos_id: Start of Sequence 토큰 ID (기본값: \"< SOS >\" 토큰 찾기)\n",
        "        eos_id: End of Sequence 토큰 ID (기본값: \"<EOS>\" 토큰 찾기)\n",
        "\n",
        "    Returns:\n",
        "        str: 생성된 응답 텍스트\n",
        "    \"\"\"\n",
        "    # SOS, EOS 토큰 ID 찾기\n",
        "    if sos_id is None:\n",
        "        sos_id = tokenizer.word_index.get(\"< SOS >\", tokenizer.word_index.get(\"<SOS>\", 2))\n",
        "    if eos_id is None:\n",
        "        eos_id = tokenizer.word_index.get(\"<EOS>\", 3)\n",
        "\n",
        "    # 입력 텍스트 전처리 및 토크나이징\n",
        "    if isinstance(input_text, str):\n",
        "        # 문자열인 경우 mecab으로 형태소 분석\n",
        "        processed_input = preprocess(input_text)\n",
        "        input_sequence = tokenizer.texts_to_sequences([processed_input])\n",
        "    else:\n",
        "        # 이미 리스트인 경우\n",
        "        input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "\n",
        "    # 패딩 적용\n",
        "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        input_sequence, padding='post'\n",
        "    )\n",
        "\n",
        "    # 배치 차원 추가 및 int32 타입으로 변환\n",
        "    encoder_input = tf.convert_to_tensor(input_sequence, dtype=tf.int32)\n",
        "\n",
        "    # 디코더 입력 초기화 (SOS 토큰으로 시작, int32 타입)\n",
        "    decoder_input = tf.convert_to_tensor([[sos_id]], dtype=tf.int32)\n",
        "\n",
        "    # 생성 루프\n",
        "    for i in range(max_length):\n",
        "        # 마스크 생성\n",
        "        enc_mask, dec_enc_mask, dec_mask = generate_masks(encoder_input, decoder_input)\n",
        "\n",
        "        # 모델 예측\n",
        "        predictions, _, _, _ = model(\n",
        "            encoder_input, decoder_input, enc_mask, dec_enc_mask, dec_mask\n",
        "        )\n",
        "\n",
        "        # 마지막 토큰의 예측값에서 가장 높은 확률의 토큰 선택\n",
        "        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1)\n",
        "\n",
        "        # int32로 타입 변환 (concat 오류 방지)\n",
        "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
        "\n",
        "        # EOS 토큰이 생성되면 종료\n",
        "        if predicted_id.numpy()[0, 0] == eos_id:\n",
        "            break\n",
        "\n",
        "        # 디코더 입력에 예측된 토큰 추가\n",
        "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
        "\n",
        "    # 생성된 시퀀스를 텍스트로 변환\n",
        "    generated_sequence = decoder_input.numpy()[0]\n",
        "\n",
        "    # SOS, EOS 토큰 제거 및 텍스트 변환\n",
        "    generated_tokens = []\n",
        "    for token_id in generated_sequence[1:]:  # SOS 토큰 제외\n",
        "        if token_id == eos_id or token_id == 0:  # EOS 또는 패딩 토큰이면 종료\n",
        "            break\n",
        "        if token_id in tokenizer.index_word:\n",
        "            generated_tokens.append(tokenizer.index_word[token_id])\n",
        "\n",
        "    return ' '.join(generated_tokens)\n",
        "\n",
        "\n",
        "# 사용 예시 함수\n",
        "def generate_response(model, tokenizer, question):\n",
        "    \"\"\"\n",
        "    질문에 대한 응답을 생성하는 편의 함수\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Transformer 모델\n",
        "        tokenizer: 텍스트 토크나이저\n",
        "        question: 질문 문자열\n",
        "\n",
        "    Returns:\n",
        "        str: 생성된 응답\n",
        "    \"\"\"\n",
        "    return greedy_decode(model, tokenizer, question)\n",
        "\n",
        "\n",
        "# 배치 처리를 위한 함수\n",
        "def greedy_decode_batch(model, tokenizer, input_texts, max_length=35, sos_id=None, eos_id=None):\n",
        "    \"\"\"\n",
        "    여러 입력에 대해 배치 처리로 greedy decoding 수행\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 Transformer 모델\n",
        "        tokenizer: 텍스트 토크나이저\n",
        "        input_texts: 입력 텍스트 리스트\n",
        "        max_length: 생성할 최대 시퀀스 길이\n",
        "        sos_id: Start of Sequence 토큰 ID\n",
        "        eos_id: End of Sequence 토큰 ID\n",
        "\n",
        "    Returns:\n",
        "        list: 생성된 응답 텍스트 리스트\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "    for text in input_texts:\n",
        "        response = greedy_decode(model, tokenizer, text, max_length, sos_id, eos_id)\n",
        "        responses.append(response)\n",
        "    return responses"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_bleu_score(reference_texts, generated_texts):\n",
        "    scores = []\n",
        "    for ref, gen in zip(reference_texts, generated_texts):\n",
        "        ref_tokens = ref.split()\n",
        "        gen_tokens = gen.split()\n",
        "        score = sentence_bleu([ref_tokens], gen_tokens, weights=(0.5, 0.5))\n",
        "        scores.append(score)\n",
        "    return sum(scores) / len(scores) if scores else 0"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ids_to_text(tokenizer, ids, pad_id=0):\n",
        "    return ' '.join([tokenizer.index_word.get(i, '') for i in ids if i != 0 and i != pad_id])\n",
        "\n",
        "def compute_bleu_from_validation(model, tokenizer, dataset, num_samples=10, sos_id=2, eos_id=3, pad_id=0):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    count = 0\n",
        "\n",
        "    for src_batch, tgt_batch in dataset:\n",
        "        for src, tgt in zip(src_batch.numpy(), tgt_batch.numpy()):\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "\n",
        "            src_text = ids_to_text(tokenizer, src)\n",
        "            tgt_text = ids_to_text(tokenizer, tgt[1:])  # <sos> 제외\n",
        "\n",
        "            pred_text = greedy_decode(model, tokenizer, src_text, sos_id=sos_id, eos_id=eos_id)\n",
        "\n",
        "            references.append(tgt_text)\n",
        "            predictions.append(pred_text)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "    return compute_bleu_score(references, predictions)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(\n",
        "    transformer,\n",
        "    optimizer,\n",
        "    name,\n",
        "    EPOCHS=10,\n",
        "    early_stopping=EarlyStopping(patience=3, min_delta=0.001),\n",
        "    train_dataset=train_dataset,\n",
        "    valid_dataset=valid_dataset,\n",
        "    save=True,\n",
        "):\n",
        "    train_log = []\n",
        "    valid_log = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "        tqdm_bar = tqdm(total=dataset_count)\n",
        "\n",
        "        for batch, (src, tgt) in enumerate(train_dataset):\n",
        "            loss, _, _, _ = train_step(src, tgt, transformer, optimizer)\n",
        "            total_loss += loss\n",
        "            tqdm_bar.set_description(f\"Epoch {epoch + 1}\")\n",
        "            tqdm_bar.set_postfix(loss=total_loss.numpy() / (batch + 1))\n",
        "            tqdm_bar.update(1)\n",
        "\n",
        "        tqdm_bar.close()\n",
        "        train_epoch_loss = total_loss.numpy() / dataset_count\n",
        "        train_log.append(train_epoch_loss)\n",
        "\n",
        "        val_loss_total = 0\n",
        "        val_batches = tf.data.experimental.cardinality(valid_dataset).numpy()\n",
        "\n",
        "        for batch, (src, tgt) in enumerate(valid_dataset):\n",
        "            loss, _, _, _ = eval_step(src, tgt, transformer)\n",
        "            val_loss_total += loss\n",
        "\n",
        "        val_epoch_loss = val_loss_total.numpy() / val_batches\n",
        "        valid_log.append(val_epoch_loss)\n",
        "\n",
        "        # BLEU 계산\n",
        "        bleu_score = compute_bleu_from_validation(transformer, tokenizer, valid_dataset, num_samples=10)\n",
        "\n",
        "        # 🔸 수정된 출력 라인: train loss, val loss, BLEU 모두 출력\n",
        "        print(f\"[Epoch {epoch + 1}] Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_epoch_loss:.4f} | BLEU: {bleu_score:.4f}\")\n",
        "\n",
        "        is_best = early_stopping.on_epoch_end(epoch, val_epoch_loss)\n",
        "        if save and is_best:\n",
        "            transformer.save_weights(name + \"best_model.weights.h5\")\n",
        "            print(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "        if early_stopping.stop_training:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    plot_loss_graph(train_log, valid_log)\n",
        "\n",
        "    if save:\n",
        "        transformer.load_weights(name + \"best_model.weights.h5\")\n",
        "\n",
        "    return transformer, train_log, valid_log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer1 = Transformer(\n",
        "    n_layers=1,\n",
        "    d_model=368,\n",
        "    n_heads=8,\n",
        "    d_ff=1024,\n",
        "    src_vocab_size=len(tokenizer.word_index) + 1,\n",
        "    tgt_vocab_size=len(tokenizer.word_index) + 1,\n",
        "    pos_len=80,\n",
        "    dropout=0.2,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True,\n",
        ")\n",
        "\n",
        "d_model = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = LearningRateScheduler(d_model, warmup_steps=1000)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(patience=4, min_delta=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer1, train_log, valid_log = main(\n",
        "    transformer1,\n",
        "    optimizer,\n",
        "    \"transformer1_\",\n",
        "    EPOCHS=10,\n",
        "    early_stopping=early_stopping,\n",
        "    train_dataset=train_dataset,\n",
        "    valid_dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def generate_answer(model, tokenizer, question, device, max_length=50):\n",
        "    # 질문과 답변 시작 토큰 조합\n",
        "    prompt = f\"질문: {question}\\n답변: \"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_length=inputs['input_ids'].size(1) + max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,  # 샘플링으로 다양성 증가 가능\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        temperature=0.8\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # 답변 부분만 추출 (질문 다음부터)\n",
        "    answer = generated_text.split(\"답변:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "def calculate_perplexity_for_answer(model, tokenizer, question, answer, device):\n",
        "    conversation = f\"질문: {question}\\n답변: {answer}\"\n",
        "\n",
        "    inputs = tokenizer(conversation, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "    question_part = f\"질문: {question}\\n답변: \"\n",
        "    question_inputs = tokenizer(question_part, return_tensors='pt')\n",
        "    question_length = question_inputs['input_ids'].size(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n",
        "\n",
        "        shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
        "        shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "        answer_mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
        "        answer_mask[:, question_length-1:] = True\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        losses = losses.view(shift_labels.shape)\n",
        "\n",
        "        masked_losses = losses * answer_mask.float()\n",
        "        answer_loss = masked_losses.sum() / answer_mask.sum()\n",
        "\n",
        "    return math.exp(answer_loss.item())\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "qa_pairs = [\n",
        "    (\"지루하다, 놀러가고 싶어.\", \"그럴 땐 가볍게 산책이라도 나가보는 건 어때요? 기분 전환에 좋을 거예요!\"),\n",
        "    (\"오늘 일찍 일어났더니 피곤하다.\", \"수고 많았네요! 잠깐 눈이라도 붙이면 좀 나아질 거예요.\"),\n",
        "    (\"간만에 여자친구랑 데이트 하기로 했어.\", \"좋은 시간 보내세요! 어디 가실 건가요?\"),\n",
        "    (\"집에 있는다는 소리야.\", \"그럼 집에서 푹 쉬거나 좋아하는 거 하면서 힐링해보세요!\")\n",
        "]\n",
        "\n",
        "print(\"=== 생성 답변 및 개별 Perplexity 출력 ===\")\n",
        "for question, ground_truth in qa_pairs:\n",
        "    generated_answer = generate_answer(model, tokenizer, question, device)\n",
        "    ppl = calculate_perplexity_for_answer(model, tokenizer, question, generated_answer, device)\n",
        "    print(f\"질문: {question}\")\n",
        "    print(f\"생성 답변: {generated_answer}\")\n",
        "    print(f\"생성 답변 Perplexity: {ppl:.2f}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "- eda 증강을 했는데 미숙한 탓에 단어가 잘려서 나왔다. 하지만 시간이 부족해서 그대로 진행했다. + 유의어 대체가 되지 않았다.\n",
        "- 그 결과 답변 생성이 아예 해석할 수 없게 나왔다. 아무래도 앞서 말한 단어가 잘린 증강 데이터들 문제인 것 같다.\n",
        "- eda 증강을 시도하려면 랜덤 삭제에서는 조사만 삭제하거나, 삽입에서는 명사와 서술어를 추가하거나 그런 식으로 진행해야 될 것 같다.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}